---
layout:     post   				    # 使用的布局（不需要改）
title:      Airflow简介  		# 标题 
subtitle:   包括一些简单示例        #副标题
date:       2020-07-12		# 时间
author:     Haiming 						# 作者
header-img: img/post-bg-2015.jpg 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - Programming
    - Airflow
---

作为一个Data Engineer，往往大家都是从ETL开始做起。那么Airflow就是ETL之中不可缺少的一个工具。

本文会对相关的基础概念进行介绍，并且搭配一些例子。

[参考](https://zhuanlan.zhihu.com/p/36043468)

# 1. 简介

先讲什么是ETL。ETL是Extract, transform, load三个词的组合，先看Wikipedia的定义：

> In [computing](https://en.wikipedia.org/wiki/Computing), **extract, transform, load** (**ETL**) is the general procedure of copying data from one or more sources into a destination system which represents the data differently from the source(s) or in a different context than the source(s). The ETL process became a popular concept in the 1970s and is often used in [data warehousing](https://en.wikipedia.org/wiki/Data_warehouse).[[1\]](https://en.wikipedia.org/wiki/Extract,_transform,_load#cite_note-1)

说白了，就是将信息从多个源传递到多个目的地，同时加入相应的处理。

Airflow就是这样的一款工具，其可以以非常灵活的方式来支持数据的ETL过程，同时孩支持插件完成比如HDFS监控（Hadoop 分布式文件系统），邮件通知等等功能。

其提供了大量的python SDK, 可以使用户在其规范下面，使用python来定义各个ETL节点之间的执行工作，节点关系；同时指定执行计划，失败策略等等，提交到Airflow之后，平台可以根据计划来**自动执行**。同时还提供了一个Web UI来查看数据流程的执行和支持一部分的简单操作。

# 2. 概念

AIrflow之中的几个主要概念：

1. `Operators`: Airflow定义的一系列算子/操作符，更直接的理解就是python class。不同的operator类实现了不同的功能，比如：
   1. `BashOperator`： 可以执行用户的一个Bash命令
   2. `PythonOperator`：可以执行用户指定的一个python函数
   3. `EmailOperator`：可以进行邮件发送
   4. `Sensor`：感知器/触发器，可以定义触发条件和动作，在条件满足的时候执行某个动作。例如`DatabaseSensor`，`FileSensor`等等。
2. `Tasks`：其就是`Operators`的具体事例，在某个`Operator`上面指定了具体内容。很像OO概念之中的对象。
3. `Task Instances`：一个Task的一次运行会产生一个实例
4. `DAGS`： 有向无环图，包括的是一系列的`tasks`和`tasks`之间的链接关系。

那么简单梳理，其实使用Airflow的过程，就是定义以上概念的过程：

1. 根据实际的需要，使用不同`Operator`
2. 传入具体的参数，定义一系列的`Tasks`
3. 定义`Tasks`之间的关系，形成一个DAG
4. 调度DAG运行，此时每一个task会生成一个task instance
5. 使用命令行或者Web UI进行管理和查看

# 3. 安装

实际上官网部分的get started主要就是教你如何安装，此处不再赘述，只是复制官网内容：

```python
# airflow needs a home, ~/airflow is the default,
# but you can lay foundation somewhere else if you prefer
# (optional)
export AIRFLOW_HOME=~/airflow

# install from pypi using pip
pip install apache-airflow

# initialize the database
airflow initdb

# start the web server, default port is 8080
airflow webserver -p 8080

# start the scheduler
airflow scheduler

# visit localhost:8080 in the browser and enable the example dag in the home page
```

